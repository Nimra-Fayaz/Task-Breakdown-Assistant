# Docker Environment Configuration for Task Breakdown Assistant
# This file contains Docker-specific environment variables
# Copy this to .env.docker and customize as needed

# =============================================================================
# AI SERVICE CONFIGURATION
# =============================================================================
# Choose: "ollama" (local, free - RECOMMENDED for Docker), "gemini", or "openai"
AI_SERVICE=ollama

# Ollama Configuration (Local AI - FREE and Recommended for Docker)
# The docker-compose.yml automatically pulls and runs llama3.2 model
OLLAMA_MODEL=llama3.2
# Note: In Docker, Ollama runs in a separate container
# Backend connects via: http://ollama:11434

# Google Gemini API (Free tier - Optional)
# Get API key from: https://aistudio.google.com/app/apikey
# GEMINI_API_KEY=your_gemini_api_key_here

# OpenAI API (Paid - Optional)
# Get API key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=your_openai_api_key_here

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
# SQLite database - stored in Docker volume for persistence
DATABASE_URL=sqlite:///./data/task_breakdown.db

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================
HOST=0.0.0.0
PORT=8000

# =============================================================================
# PYTHON CONFIGURATION
# =============================================================================
PYTHONUNBUFFERED=1
PYTHONDONTWRITEBYTECODE=1

# =============================================================================
# NOTES
# =============================================================================
# - This file is used by docker-compose.yml
# - Don't commit sensitive API keys to version control
# - For production, use Docker secrets or environment variables
# - Ollama data is persisted in Docker volume: task-breakdown-ollama-data
# - Backend data is persisted in Docker volume: task-breakdown-backend-data
